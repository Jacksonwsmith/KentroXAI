<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Trust Metrics Spec</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Arial, sans-serif; margin: 0; background: #f6f8fb; color: #1b2230; }
  .wrap { max-width: 1200px; margin: 24px auto; padding: 20px 24px; background: #fff; border: 1px solid #d9dee9; border-radius: 12px; }
  h1 { font-size: 30px; margin: 0 0 12px; }
  h2 { font-size: 22px; margin: 22px 0 8px; }
  h3 { font-size: 18px; margin: 16px 0 8px; }
  p { margin: 6px 0; line-height: 1.45; }
  ul, ol { margin: 8px 0 12px 22px; line-height: 1.45; }
  table { width: 100%; border-collapse: collapse; margin: 10px 0 16px; font-size: 13px; }
  th, td { border: 1px solid #d9dee9; padding: 7px 8px; text-align: left; vertical-align: top; }
  th { background: #eef2f9; font-weight: 700; }
</style>
</head>
<body>
  <div class="wrap"><h1>Trust Metrics Specification (Current State)</h1><h2>Why This Document</h2><p>This document defines the metrics currently used to establish trust in the toolkit, where each metric comes from, how pass/fail is computed, and how it affects release decisions.</p><h2>1) Metrics Currently in Use</h2><table><thead><tr><th>Metric ID</th><th>Dimension</th><th>Current Source</th><th>Default Threshold</th><th>Pass Logic</th><th>Importance to Trust</th></tr></thead><tbody><tr><td>`accuracy_stub`</td><td>Quality</td><td>Deterministic stub value (`0.81`)</td><td>`0.7` (configurable)</td><td>`value &gt;= threshold`</td><td>Baseline model correctness signal.</td></tr><tr><td>`reliability`</td><td>Reliability</td><td>Deterministic stub value (`0.83`)</td><td>`0.75` (configurable)</td><td>`value &gt;= threshold`</td><td>Indicates stability/consistency.</td></tr><tr><td>`groundedness_stub`</td><td>Grounding</td><td>Deterministic stub value (`0.72`)</td><td>`0.6` (configurable)</td><td>`value &gt;= threshold`</td><td>Indicates alignment to retrieval/context.</td></tr><tr><td>`refusal_correctness`</td><td>Safety</td><td>Derived from suite case mix (`unsafe_cases/total_cases`)</td><td>`0.8` (configurable)</td><td>`value &gt;= threshold`</td><td>Measures correct refusal behavior in unsafe contexts.</td></tr><tr><td>`unanswerable_handling`</td><td>Safety</td><td>Derived from suite case mix (`unanswerable_cases/total_cases`)</td><td>`0.78` (configurable)</td><td>`value &gt;= threshold`</td><td>Measures uncertainty handling and non-fabrication behavior.</td></tr><tr><td>`fairness_demographic_parity_diff`</td><td>Fairness</td><td>Synthetic cohort labels (AIF360-inspired formula)</td><td>`0.2` (configurable)</td><td>`abs(value) &lt;= threshold`</td><td>Detects group selection-rate disparity.</td></tr><tr><td>`fairness_disparate_impact_ratio`</td><td>Fairness</td><td>Synthetic cohort labels (AIF360-inspired formula)</td><td>`0.8` (configurable)</td><td>`value &gt;= threshold`</td><td>Enforces adverse-impact ratio baseline.</td></tr><tr><td>`fairness_equal_opportunity_difference`</td><td>Fairness</td><td>Synthetic cohort labels (AIF360-inspired formula)</td><td>`0.2` (configurable)</td><td>`abs(value) &lt;= threshold`</td><td>Compares TPR parity across groups.</td></tr><tr><td>`fairness_average_odds_difference`</td><td>Fairness</td><td>Synthetic cohort labels (AIF360-inspired formula)</td><td>`0.2` (configurable)</td><td>`abs(value) &lt;= threshold`</td><td>Compares average FPR/TPR parity.</td></tr></tbody></table><h2>2) What Actually Drives Governance Decisions</h2><h3>Evaluation Gate</h3><ul><li>`evaluation = fail` if **any metric has `passed = false`**.</li><li>Otherwise `evaluation = pass`.</li></ul><h3>Red-Team Gate</h3><ul><li>`redteam = needs_review` when high/critical findings exist.</li><li>`redteam = fail` when risk rules require red-team and findings are missing, or when high/critical findings are blocking for that risk tier.</li></ul><h3>Documentation Gate</h3><ul><li>`documentation = pass` when evidence completeness &gt;= `90%`; otherwise `needs_review`.</li></ul><h3>Overall Trust Decision</h3><ul><li>If any stage gate is `fail` -&gt; `overall_status = fail`, `go_no_go = no-go`.</li><li>Else if any stage gate is `needs_review` -&gt; `overall_status = needs_review`, `go_no_go = no-go`.</li><li>Else -&gt; `overall_status = pass`, `go_no_go = go`.</li></ul><h2>3) Trust Score in HTML Card (UI Layer)</h2><p>The current Trust Card UI computes a display score as:</p><p>trust_score = 92 - (blocker_count * 9) - ((100 - evidence_completeness) / 4)</p><p>Where:</p><ul><li>`blocker_count = redteam_high + redteam_critical + failed_metric_count`</li><li>score is clamped to `[35, 96]`</li></ul><p>Important: this is currently a presentation heuristic in the HTML template, not a backend governance contract.</p><h2>4) Current Gaps (Directly Related to Your Concern)</h2><ol><li>Most evaluation metrics are currently deterministic stubs or synthetic fairness cohorts.</li><li>Metrics are not yet fully computed from real model prediction/output logs.</li><li>Trust decisions are structurally correct, but some inputs are placeholders.</li></ol><h2>5) Priority Next Steps to Make Trust Real</h2><ol><li>Replace `accuracy_stub` and `reliability` with dataset-backed computations from real outputs.</li><li>Replace fairness synthetic cohorts with run-specific cohort labels from evaluation data.</li><li>Replace `groundedness_stub` with citation/context overlap scoring from prompt/output/context artifacts.</li><li>Keep existing gate logic, but treat stub metrics as non-production until replaced.</li><li>Version the trust-scoring contract so UI score and backend decision logic are explicitly synchronized.</li></ol><h2>6) Source of Truth in Code</h2><ul><li>Metric definitions: `/Users/ethanhall/Kentro_Workspace/src/trusted_ai_toolkit/eval/metrics/__init__.py`</li><li>Fairness formulas: `/Users/ethanhall/Kentro_Workspace/src/trusted_ai_toolkit/eval/metrics/aif360_compat.py`</li><li>Decision logic: `/Users/ethanhall/Kentro_Workspace/src/trusted_ai_toolkit/reporting.py`</li><li>UI trust-score formula: `/Users/ethanhall/Kentro_Workspace/src/trusted_ai_toolkit/templates/scorecard.html.j2`</li></ul></div>
</body>
</html>
